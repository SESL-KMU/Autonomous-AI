{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL18qoWSf_rf"
      },
      "source": [
        "#<font color=blue>**YOLOë¥¼ í†µí•œ ë²„í´ë¦¬ ë°ì´í„°ì…‹ í•™ìŠµ**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRVhEx5wf_t3"
      },
      "source": [
        "##<font color=green>**ë²„í´ë¦¬ ë°ì´í„° ì—…ë¡œë“œ ì§„í–‰**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!gdown 13azWx8p0dMjrqwmSk_904XTsXFu9QTyt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "box-lD14f_yl"
      },
      "source": [
        "####<font color=red>ì—…ë¡œë“œí•œ BDD_data.zip ì••ì¶•í’€ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_R2sBFFluTz",
        "outputId": "80257f59-f266-4e82-bea8-7deedfb8bc34"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'unzip'ï¿½ï¿½(ï¿½ï¿½) ï¿½ï¿½ï¿½ï¿½ ï¿½Ç´ï¿½ ï¿½Üºï¿½ ï¿½ï¿½ï¿½ï¿½, ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ ï¿½Ö´ï¿½ ï¿½ï¿½ï¿½Î±×·ï¿½, ï¿½Ç´ï¿½\n",
            "ï¿½ï¿½Ä¡ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½Æ´Õ´Ï´ï¿½.\n"
          ]
        }
      ],
      "source": [
        "!unzip /content/DataSet.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "im0onX-Lf_1j"
      },
      "source": [
        "####<font color=red>ê°ê°ì˜ ë°ì´í„° ê°œìˆ˜ í™•ì¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJAu4oFElvxp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "images_train=os.listdir('/content/DataSet/images/train/')\n",
        "images_val=os.listdir('/content/DataSet/images/val/')\n",
        "\n",
        "labels_train=os.listdir('/content/DataSet/labels/train/')\n",
        "labels_val=os.listdir('/content/DataSet/labels/val/')\n",
        "\n",
        "\n",
        "print(f'train images ê°œìˆ˜:{len(images_train)}')\n",
        "print(f'validation images ê°œìˆ˜:{len(images_val)}')\n",
        "print(\"\")\n",
        "print(f'train labels ê°œìˆ˜:{len(labels_train)}')\n",
        "print(f'validation labels ê°œìˆ˜:{len(labels_val)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cJs7FPClXfk"
      },
      "source": [
        "##<font color=green>**gitì˜ YOLOv5 ì„¤ì¹˜**</font>\n",
        "ì„¤ì¹˜ í›„ custom_data.yaml íŒŒì¼ì„ yolov5/data í´ë”ì— ì—…ë¡œë“œí•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vq17GQN7lxGu",
        "outputId": "98366dd0-bf39-4193-d3a3-5aff3506303a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup complete. Using torch 1.9.0+cu102 (CPU)\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "%cd yolov5\n",
        "%cp ../custom_data.yaml ./data/\n",
        "%pip install -qr requirements.txt  # install dependencies\n",
        "\n",
        "import torch\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "clear_output()\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2bMufFDlXh5"
      },
      "source": [
        "##<font color=green>**í•™ìŠµ ì‹œì‘**</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJbSvJV6lyM8",
        "outputId": "6fb45512-8362-43af-9c75-6789e1e18ccd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5x.pt, cfg=, data=custom_data.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=10, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, project=runs/train, entity=None, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, upload_dataset=False, bbox_interval=-1, save_period=-1, artifact_alias=latest, local_rank=-1, freeze=0\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n",
            "YOLOv5 ğŸš€ v5.0-351-ge96c74b torch 1.9.0+cu102 CPU\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.2, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 ğŸš€ runs (RECOMMENDED)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "2021-08-07 05:09:15.142004: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "Overriding model.yaml nc=80 with nc=10\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      8800  models.common.Focus                     [3, 80, 3]                    \n",
            "  1                -1  1    115520  models.common.Conv                      [80, 160, 3, 2]               \n",
            "  2                -1  1    309120  models.common.C3                        [160, 160, 4]                 \n",
            "  3                -1  1    461440  models.common.Conv                      [160, 320, 3, 2]              \n",
            "  4                -1  1   3285760  models.common.C3                        [320, 320, 12]                \n",
            "  5                -1  1   1844480  models.common.Conv                      [320, 640, 3, 2]              \n",
            "  6                -1  1  13125120  models.common.C3                        [640, 640, 12]                \n",
            "  7                -1  1   7375360  models.common.Conv                      [640, 1280, 3, 2]             \n",
            "  8                -1  1   4099840  models.common.SPP                       [1280, 1280, [5, 9, 13]]      \n",
            "  9                -1  1  19676160  models.common.C3                        [1280, 1280, 4, False]        \n",
            " 10                -1  1    820480  models.common.Conv                      [1280, 640, 1, 1]             \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1   5332480  models.common.C3                        [1280, 640, 4, False]         \n",
            " 14                -1  1    205440  models.common.Conv                      [640, 320, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1   1335040  models.common.C3                        [640, 320, 4, False]          \n",
            " 18                -1  1    922240  models.common.Conv                      [320, 320, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1   4922880  models.common.C3                        [640, 640, 4, False]          \n",
            " 21                -1  1   3687680  models.common.Conv                      [640, 640, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1  19676160  models.common.C3                        [1280, 1280, 4, False]        \n",
            " 24      [17, 20, 23]  1    100935  models.yolo.Detect                      [10, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [320, 640, 1280]]\n",
            "Model Summary: 607 layers, 87304935 parameters, 87304935 gradients, 217.5 GFLOPs\n",
            "\n",
            "Transferred 788/794 items from yolov5x.pt\n",
            "Scaled weight_decay = 0.0005\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 131 weight, 134 weight (no decay), 134 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../DataSet/labels/train.cache' images and labels... 4200 found, 0 missing, 0 empty, 0 corrupted: 100% 4200/4200 [00:00<00:00, 33942344.51it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (2.9GB ram): 100% 4200/4200 [00:41<00:00, 100.29it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '../DataSet/labels/val.cache' images and labels... 1200 found, 0 missing, 0 empty, 0 corrupted: 100% 1200/1200 [00:00<00:00, 7069051.69it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.8GB ram): 100% 1200/1200 [00:13<00:00, 91.52it/s]\n",
            "Plotting labels... \n",
            "\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 2.46, Best Possible Recall (BPR) = 0.8836. Attempting to improve anchors, please wait...\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mWARNING: Extremely small objects found. 6100 of 78218 labels are < 3 pixels in size.\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mRunning kmeans for 9 anchors on 78218 points...\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mthr=0.25: 0.9985 best possible recall, 4.48 anchors past thr\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mn=9, img_size=640, metric_all=0.303/0.722-mean/best, past_thr=0.486-mean: 14,4,  22,9,  41,7,  40,18,  98,14,  65,31,  116,49,  198,78,  334,121\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7532: 100% 1000/1000 [00:26<00:00, 37.11it/s]\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mthr=0.25: 0.9994 best possible recall, 5.40 anchors past thr\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mn=9, img_size=640, metric_all=0.352/0.753-mean/best, past_thr=0.497-mean: 12,3,  14,6,  25,5,  23,9,  53,7,  37,15,  62,27,  115,44,  213,87\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mNew anchors saved to model. Update model *.yaml to use these anchors in the future.\n",
            "\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to runs/train/exp2\n",
            "Starting training for 10 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "  0% 0/263 [00:00<?, ?it/s]^C\n"
          ]
        }
      ],
      "source": [
        "# Train YOLOv5s on custom_data for 20 epochs\n",
        "!python train.py --img 320 --batch 16 --epochs 2 --data custom_data.yaml --weights yolov5s.pt --cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WN0bIYDEl0nw"
      },
      "outputs": [],
      "source": [
        "!ls runs/train/exp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeeaD-nolXkI"
      },
      "source": [
        "####<font color=red>í•™ìŠµëœ Loss ê·¸ë˜í”„ í™•ì¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3WxCoZ7l1--"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image as Display # to display images\n",
        "# expì˜ ê²½ìš° ì‹¤í–‰ë§ˆë‹¤ 1ì”© ì¶”ê°€ë˜ì–´ ìƒì„±\n",
        "Display(filename='runs/train/exp/results.png', width=1080) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ2GKSHVlf1k"
      },
      "source": [
        "####<font color=red>í…ì„œë³´ë“œë¥¼ í†µí•´ Loss ê·¸ë˜í”„ í™•ì¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPfWkw2Ql3Qp"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "# TensorBoard ì¤€ë¹„í•˜ê¸°\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2YDU8SWliWi"
      },
      "source": [
        "####<font color=red>ì›í•˜ëŠ” ì´ë¯¸ì§€ë¡œ í…ŒìŠ¤íŠ¸</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rXgeTHal4nk"
      },
      "outputs": [],
      "source": [
        "# Evaluation with the validation data\n",
        "# ì›í•˜ëŠ” ì´ë¯¸ì§€ì˜ ê²½ë¡œì™€ íŒŒì¼ëª…ì„ ì§€ì •í•˜ì—¬ ì‹¤í–‰\n",
        "!python detect.py --weights ./runs/train/exp/weights/best.pt --img 640 --conf 0.25 --source ../DataSet/images/val/4201.jpg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcfckCOukLH2"
      },
      "source": [
        "##<font color=green>**Weight ê²€ì¦**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAP1jIKBkLhY",
        "outputId": "138ea60f-7c6d-4e59-9bd0-cb052122efde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mdata=./data/custom_data.yaml, weights=['test/s_best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.65, task=val, device=, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=True\n",
            "YOLOv5 ğŸš€ v5.0-351-ge96c74b torch 1.9.0+cu102 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 224 layers, 7078183 parameters, 0 gradients, 16.4 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '../DataSet/labels/val' images and labels...1200 found, 0 missing, 0 empty, 0 corrupted: 100% 1200/1200 [00:01<00:00, 700.62it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: ../DataSet/labels/val.cache\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 38/38 [05:57<00:00,  9.40s/it]\n",
            "                 all       1200      21736      0.634      0.329      0.343      0.155\n",
            "        traffic sign       1200       3327      0.536      0.402      0.381      0.111\n",
            "       traffic light       1200       3988      0.609      0.462      0.469      0.209\n",
            "                 car       1200      11739      0.658      0.644      0.668       0.36\n",
            "               rider       1200       1692      0.573      0.364      0.375      0.128\n",
            "               motor       1200        211      0.502      0.299      0.265       0.17\n",
            "              person       1200        520      0.621       0.35      0.377      0.237\n",
            "                 bus       1200         84      0.645     0.0476      0.106     0.0412\n",
            "               truck       1200        124      0.591      0.314      0.318      0.108\n",
            "                bike       1200         51      0.967     0.0784      0.127     0.0312\n",
            "Speed: 1.3ms pre-process, 287.7ms inference, 3.6ms NMS per image at shape (32, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/val/exp6\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python val.py --weights ./runs/train/exp/weights/best.pt --data custom_data.yaml --img 640 --iou 0.6 --half"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHHp1OHVlk12"
      },
      "source": [
        "##<font color=green>**ëŒ€íšŒ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ**</font>\n",
        "Loss ê·¸ë˜í”„\n",
        "\n",
        "yaml íŒŒì¼\n",
        "\n",
        "í•™ìŠµì¤‘ ë² ìŠ¤íŠ¸ ëª¨ë¸\n",
        "\n",
        "<<ê²½ë¡œ ì£¼ì˜>>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qicSZkfol-We"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('/content/yolov5/runs/train/exp/results.png')\n",
        "files.download('/content/yolov5/data/custom_data.yaml')\n",
        "files.download('/content/yolov5/runs/train/exp/weights/best.pt')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "YOLOv5_ê°ì²´_ì¸ì‹_ëŒ€íšŒ_ë°°í¬.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
